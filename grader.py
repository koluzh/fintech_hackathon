from openai import OpenAI
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from time import perf_counter
import argparse
import asyncio
import os
import pandas as pd

load_dotenv()
LLM_API_KEY = os.getenv("LLM_API_KEY")

class GradeResponse(BaseModel):
    grade: int = Field(..., description="ĞÑ†ĞµĞ½ĞºĞ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°")
    reasoning: str = Field(..., description="ĞĞ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸")

class Grader:
    def __init__(self, grading_model):
        self.grading_model = grading_model
        self.client = OpenAI(base_url="https://openrouter.ai/api/v1",api_key=LLM_API_KEY)
     
    def get_system_prompt(self):
        return f"""
    Ğ¢ĞµĞ±Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° - Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚ĞµĞ½ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ·Ğ°Ğ´Ğ°Ğ²ÑˆĞµĞ¼Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ.

    Ğ”Ğ°Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 5:
    1: Ğ‘ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ»Ğ¸Ğ±Ğ¾ Ğ½Ğµ ÑĞ²ÑĞ·Ğ°Ğ½ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼
    2: ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ ÑƒĞ¿ÑƒÑ‰ĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ 
    3: ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¹, Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚
    4: Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, ĞµÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ
    5: ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ

    Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ğ²Ğ¸Ğ´Ğµ:

    ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ:::
    ĞÑ†ĞµĞ½ĞºĞ°: (Ñ‚Ğ²Ğ¾Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 5)
    ĞĞ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: (Ğ¾Ğ¿Ğ¸ÑˆĞ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°)

    """.strip()

    def get_grading_prompt(self, question, answer_for_grading):
        return f"""
    Ğ”Ğ°Ğ½Ñ‹ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚:

    ```
    Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}
    ĞÑ‚Ğ²ĞµÑ‚: {answer_for_grading}
    ```

    ĞÑ†ĞµĞ½Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ ÑˆĞºĞ°Ğ»Ğµ 1 Ğ´Ğ¾ 5, Ğ¸ Ğ´Ğ°Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ
        """.strip()

    def send_answer(self, question, answer):
        response = self.client.chat.completions.parse(
                model = self.grading_model,
                messages = [
                    {
                        "role": "system",
                        "content": self.get_system_prompt()
                    },
                    {
                        "role": "user",
                        "content": self.get_grading_prompt(question, answer)
                    }
                ],
                response_format=GradeResponse
        )

        return response.choices[0].message.parsed

    async def grade_answer(self, question, answer):
        try:
            response: GradeResponse = self.send_answer(question, answer)
        except Exception as e:
            return print(e, response)
        
        return {
            "question": question,
            "answer": answer,
            "grade" : response.grade,
            "reasoning": response.reasoning
        }

    async def start(self, df, batch_size: int = 3, output_path: str = 'grades.csv'):
        questions = df['question'].tolist()
        answers = df['result'].tolist()

        results = []

        if len(questions) != len(answers):
            raise "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚"
                
        batches_total = (len(questions) + batch_size - 1) // batch_size

        for batch_idx, i in enumerate(range(0, len(questions), batch_size), start=1):
            batch = questions[i : i + batch_size], answers[i : i + batch_size]

            print(f"ĞĞ°Ñ‡Ğ°Ğ»ÑÑ Ğ±Ğ°Ñ‚Ñ‡ {batch_idx}/{batches_total} (Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ {i + 1}-{min(i + batch_size, len(questions))})")
            print(f"Batch: {batch}")

            batch_grades = await asyncio.gather(
                *[self.grade_answer(question, answer) for question , answer in zip(*batch)]
            )

            results.extend(batch_grades)
            pd.DataFrame(results).to_csv(output_path, index=False)

            print(
            f"Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ Ğ±Ğ°Ñ‚Ñ‡ {batch_idx}/{batches_total}."
            f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {len(results)}/{len(questions)}"
            )

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ĞÑ†ĞµĞ½ĞºĞ°")

    parser.add_argument(
        "--input_path",
        type=str,
        required=False,
        default="submission.csv",
        help="CSV Ñ„Ğ°Ğ¹Ğ» Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
    )

    parser.add_argument(
        "--output_path",
        type=str,
        required=False,
        default="grades.csv",
        help="ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°",
    )

    parser.add_argument(
        "--batch_size",
        type=int,
        required=False,
        default="3",
        help="Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ°",
    )

    parser.add_argument(
        "--grading_model",
        type=str,
        required=False,
        default="qwen/qwen3-vl-8b-instruct",
        help="ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ",
    )

    parser.add_argument(
        "--grade_threshold",
        type=int,
        required=False,
        default="3",
        help="Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸",
    )

    args = parser.parse_args()
    input_path = args.input_path
    output_path = args.output_path
    batch_size = args.batch_size
    grading_model = args.grading_model
    grade_threshold = args.grade_threshold

    if os.path.exists(input_path):
        df = pd.read_csv(input_path)
        columns = ['', 'question', 'result']
        test_data = pd.DataFrame(df, columns=columns)
    try:
        grader = Grader(grading_model)
    except ValueError as e:
        print(f"âŒ {e}")

    start = perf_counter()
    asyncio.run(grader.start(df=test_data, batch_size=batch_size, output_path=output_path))
    stop = perf_counter()

    print("time taken: ", stop - start)
    if os.path.exists(output_path):
        results = pd.read_csv(output_path)

        passed = results[results['grade'] >= grade_threshold].count()
        failed = results[results['grade'] < grade_threshold].count()
        missing = results[results['grade'] == ''].count()    

        print(f"ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:")
        print(f"   â€¢ ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {grading_model}")
        print(f"   â€¢ Ğ’ÑĞµĞ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {len(results)}")
        print(f"   â€¢ ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑˆĞµĞ´ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ: {passed} ({passed/len(results)*100:.1f}%)")
        print(f"   â€¢ ĞÑ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ: {failed} ({passed/len(results)*100:.1f}%)")
        print(f"   â€¢ ĞĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: {missing} ({missing/len(results)*100:.1f}%)")