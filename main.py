import pandas as pd
import numpy as np
import faiss
from tqdm import tqdm
import requests
import time
import warnings
import argparse
import json
import asyncio
import aiohttp
import re
import os
from dotenv import load_dotenv
warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class FinancialAssistant:
    def __init__(self):
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            raise ValueError("OPENROUTER_API_KEY not found in environment variables")
        
        self.embedding_model = "openai/text-embedding-3-small"
        self.generation_model = "meta-llama/llama-3-70b-instruct"
        self.embedding_dim = 1536
        
        self.index = None
        self.documents = []
        
        # –†–∞–∑–¥–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.embedding_requests = 0
        self.generation_requests = 0
        self.last_embedding_time = time.time()
        self.last_generation_time = time.time()

    async def get_embeddings_async(self, texts: list, session: aiohttp.ClientSession) -> np.ndarray:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ª–∏–º–∏—Ç–∞–º–∏"""
        embeddings = []
        
        async def fetch_embedding(text: str):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (60 –≤ –º–∏–Ω—É—Ç—É)
                current_time = time.time()
                if current_time - self.last_embedding_time > 60:
                    self.embedding_requests = 0
                    self.last_embedding_time = current_time
                
                if self.embedding_requests >= 50:  # –ë–µ—Ä–µ–º 50 –≤–º–µ—Å—Ç–æ 60 –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                    wait_time = 60 - (current_time - self.last_embedding_time) + 1
                    if wait_time > 0:
                        print(f"‚ö†Ô∏è –õ–∏–º–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –û–∂–∏–¥–∞–Ω–∏–µ {wait_time:.1f} —Å–µ–∫—É–Ω–¥...")
                        await asyncio.sleep(wait_time)
                    self.embedding_requests = 0
                    self.last_embedding_time = time.time()
                
                self.embedding_requests += 1
                
                async with session.post(
                    "https://openrouter.ai/api/v1/embeddings",
                    headers={
                        "Authorization": f"Bearer {self.openrouter_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.embedding_model,
                        "input": text[:1500]
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        if 'data' in data and len(data['data']) > 0:
                            return data['data'][0]['embedding']
                    elif response.status == 429:
                        error_data = await response.text()
                        print(f"‚ö†Ô∏è Rate limit exceeded: {error_data}")
                        await asyncio.sleep(10)
                        return None
                    else:
                        error_data = await response.text()
                        print(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ {response.status}: {error_data}")
                        return None
                        
            except Exception as e:
                print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(e)}")
                return None
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        semaphore = asyncio.Semaphore(3)  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        async def bounded_fetch(text):
            async with semaphore:
                return await fetch_embedding(text)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        with tqdm(total=len(texts), desc="–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤") as pbar:
            tasks = []
            for text in texts:
                task = asyncio.create_task(bounded_fetch(text))
                task.add_done_callback(lambda x: pbar.update(1))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception) or result is None:
                # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∫–∞—á–µ—Å—Ç–≤–µ fallback
                random_embedding = np.random.normal(0, 0.1, self.embedding_dim).tolist()
                embeddings.append(random_embedding)
                print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {i+1}")
            else:
                embeddings.append(result)
                successful += 1
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {successful}/{len(texts)}")
        return np.array(embeddings, dtype=np.float32)

    def _split_text_into_chunks(self, text: str, chunk_size: int = 800, overlap: int = 100) -> list:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"""
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 50]
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–∞–º –ø–æ —Å–µ–±–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ —á–∞–Ω–∫
            if len(paragraph) >= chunk_size // 2:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –∞–±–∑–∞—Ü –Ω–∞ —á–∞—Å—Ç–∏
                words = paragraph.split()
                for i in range(0, len(words), chunk_size - overlap):
                    chunk = ' '.join(words[i:i + chunk_size])
                    if len(chunk) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
                        chunks.append(chunk)
            else:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∞–±–∑–∞—Ü—ã
                if len(current_chunk + " " + paragraph) <= chunk_size:
                    current_chunk += " " + paragraph
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç

    def build_knowledge_base(self, train_data_path: str, max_documents: int = 500):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        train_data = pd.read_csv(train_data_path)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if len(train_data) > max_documents:
            print(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ {max_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(train_data)}")
            train_data = train_data.head(max_documents)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Å —É–º–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
        all_texts = []
        for _, row in tqdm(train_data.iterrows(), total=len(train_data), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
            text = row['text']
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = re.sub(r'#{1,6}\s*', '', text)
            cleaned_text = re.sub(r'\*\*(.*?)\*\*', r'\1', cleaned_text)
            cleaned_text = re.sub(r'\n+', ' ', cleaned_text)
            
            # –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._split_text_into_chunks(cleaned_text)
            all_texts.extend(chunks)
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Ä–∞–∑—É–º–Ω–æ–≥–æ –ø—Ä–µ–¥–µ–ª–∞
            if len(all_texts) >= 800:
                break
        
        self.documents = all_texts
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_texts)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(train_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        if not all_texts:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
            return
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        async def build_embeddings():
            async with aiohttp.ClientSession() as session:
                return await self.get_embeddings_async(all_texts, session)
        
        embeddings = asyncio.run(build_embeddings())
        
        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype(np.float32))
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")

    async def search_relevant_documents_async(self, query: str, session: aiohttp.ClientSession, top_k: int = 3) -> list:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.index is None:
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self.get_embeddings_async([query], session)
            if len(query_embedding) == 0:
                return []
            
            faiss.normalize_L2(query_embedding)
            similarities, indices = self.index.search(query_embedding.astype(np.float32), top_k)
            
            relevant_docs = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.documents) and similarities[0][i] > 0.1:
                    relevant_docs.append(self.documents[idx])
            
            return relevant_docs
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

    async def generate_answer_async(self, question: str, context_docs: list, session: aiohttp.ClientSession) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ª–∏–º–∏—Ç–∞–º–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            current_time = time.time()
            if current_time - self.last_generation_time > 60:
                self.generation_requests = 0
                self.last_generation_time = current_time
            
            if self.generation_requests >= 50:  # 50 –≤–º–µ—Å—Ç–æ 60 –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                wait_time = 60 - (current_time - self.last_generation_time) + 1
                if wait_time > 0:
                    print(f"‚ö†Ô∏è –õ–∏–º–∏—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –û–∂–∏–¥–∞–Ω–∏–µ {wait_time:.1f} —Å–µ–∫—É–Ω–¥...")
                    await asyncio.sleep(wait_time)
                self.generation_requests = 0
                self.last_generation_time = time.time()
            
            self.generation_requests += 1
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = "\n".join([f"- {doc}" for i, doc in enumerate(context_docs[:2])])
            
            prompt = f"""–¢—ã - —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ –∏ –ø–æ–ª–µ–∑–Ω–æ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
- –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–∞–∫ –æ—Å–Ω–æ–≤—É
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–∞–π –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–º

–û—Ç–≤–µ—Ç:"""

            async with session.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/financial-assistant",
                    "X-Title": "Financial Assistant"
                },
                json={
                    "model": self.generation_model,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "max_tokens": 500,
                    "temperature": 0.3
                },
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    answer = result['choices'][0]['message']['content'].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
                    if len(answer) < 25 or "–Ω–µ –º–æ–≥—É" in answer.lower() or "–Ω–µ –∑–Ω–∞—é" in answer.lower():
                        return "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–∞—Ç—å —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."
                    
                    return answer
                    
                elif response.status == 429:
                    error_data = await response.text()
                    print(f"‚ö†Ô∏è Rate limit exceeded for generation: {error_data}")
                    await asyncio.sleep(15)
                    return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                    
                else:
                    error_data = await response.text()
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {response.status}: {error_data}")
                    return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
                    
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

    async def process_questions_batch(self, questions_batch: list, progress_callback=None) -> list:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        answers = []
        
        async with aiohttp.ClientSession() as session:
            for i, question in enumerate(questions_batch):
                try:
                    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    relevant_docs = await self.search_relevant_documents_async(question, session)
                    
                    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    if i < 2:
                        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞ {i+1}:")
                        print(f"   –í–æ–ø—Ä–æ—Å: {question}")
                        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs)}")
                    
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                    answer = await self.generate_answer_async(question, relevant_docs, session)
                    answers.append(answer)
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if progress_callback:
                        progress_callback()
                        
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
                    answers.append("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞.")
                    if progress_callback:
                        progress_callback()
        
        return answers

def main():
    parser = argparse.ArgumentParser(description='–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å OpenRouter - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π')
    parser.add_argument('--num_questions', type=int, default=10,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--max_documents', type=int, default=300,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--skip_build', action='store_true',
                       help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
    parser.add_argument('--batch_size', type=int, default=3,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ –§–ò–ù–ê–ù–°–û–í–´–ô –ê–°–°–ò–°–¢–ï–ù–¢ –° OPENROUTER - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API –∫–ª—é—á–∞
    if not os.getenv("OPENROUTER_API_KEY"):
        print("‚ùå –û—à–∏–±–∫–∞: OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        print("   –î–æ–±–∞–≤—å—Ç–µ –≤–∞—à API –∫–ª—é—á –≤ —Ñ–∞–π–ª .env –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    try:
        assistant = FinancialAssistant()
    except ValueError as e:
        print(f"‚ùå {e}")
        return
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if not args.skip_build:
        print("\nüì¶ –≠—Ç–∞–ø 1: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        build_start = time.time()
        assistant.build_knowledge_base('./train_data.csv', max_documents=args.max_documents)
        build_time = time.time() - build_start
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∑–∞ {build_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    print("\nüìã –≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")
    questions_df = pd.read_csv('./questions.csv')
    questions_list = questions_df['–í–æ–ø—Ä–æ—Å'].tolist()
    
    if args.num_questions > 0:
        questions_list = questions_list[:args.num_questions]
        questions_df = questions_df.head(args.num_questions)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions_list)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    print(f"\nüéØ –≠—Ç–∞–ø 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(questions_list)} –≤–æ–ø—Ä–æ—Å–æ–≤...")
    
    async def process_all_questions():
        all_answers = []
        total_questions = len(questions_list)
        
        with tqdm(total=total_questions, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤") as pbar:
            for i in range(0, total_questions, args.batch_size):
                batch = questions_list[i:i + args.batch_size]
                
                batch_answers = await assistant.process_questions_batch(
                    batch, 
                    progress_callback=lambda: pbar.update(1)
                )
                all_answers.extend(batch_answers)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                if i + args.batch_size < total_questions:
                    await asyncio.sleep(3)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–∞—É–∑—É
        
        return all_answers
    
    process_start = time.time()
    answers_list = asyncio.run(process_all_questions())
    process_time = time.time() - process_start
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüíæ –≠—Ç–∞–ø 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    questions_df['–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å'] = answers_list
    output_file = f'submission_openrouter_optimized_{len(questions_list)}.csv'
    questions_df.to_csv(output_file, index=False, encoding='utf-8')
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    total_time = process_time + (0 if args.skip_build else build_time)
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
    excellent = sum(1 for ans in answers_list if len(ans) > 80 and "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç" not in ans.lower())
    good = sum(1 for ans in answers_list if 40 <= len(ans) <= 80 and "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç" not in ans.lower())
    basic = len(answers_list) - excellent - good
    
    print("=" * 60)
    print("üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions_list)}")
    print(f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã: {excellent} ({excellent/len(questions_list)*100:.1f}%)")
    print(f"   ‚Ä¢ –•–æ—Ä–æ—à–∏–µ –æ—Ç–≤–µ—Ç—ã: {good} ({good/len(questions_list)*100:.1f}%)")
    print(f"   ‚Ä¢ –ë–∞–∑–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã: {basic} ({basic/len(questions_list)*100:.1f}%)")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {process_time/60:.1f} –º–∏–Ω—É—Ç")
    if not args.skip_build:
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã: {build_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"   ‚Ä¢ –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_file}")
    print("=" * 60)
    
    # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–≤–µ—Ç–æ–≤
    print("\nüìù –ü—Ä–∏–º–µ—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:")
    print("-" * 80)
    
    sample_indices = list(range(min(5, len(questions_list))))
    for i in sample_indices:
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å {i+1}: {questions_list[i]}")
        print(f"üí° –û—Ç–≤–µ—Ç: {answers_list[i]}")
        print("-" * 80)

if __name__ == "__main__":
    main()