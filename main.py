import pandas as pd
import numpy as np
import faiss
from tqdm import tqdm
import requests
import time
import warnings
import argparse
import json
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import os
warnings.filterwarnings('ignore')

class FinancialAssistant:
    def __init__(self, ollama_url="http://localhost:11434"):
        self.ollama_url = ollama_url
        self.index = None
        self.documents = []
        self.embedding_dim = 768
        
        self._check_ollama_available()
    
    def _check_ollama_available(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            if response.status_code == 200:
                print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
                return True
            else:
                print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É")
                return False
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É: {e}")
            return False

    async def get_embeddings_async(self, texts: list, session: aiohttp.ClientSession, model_name: str = "embeddinggemma:300m") -> np.ndarray:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        embeddings = []
        
        async def fetch_embedding(text: str):
            try:
                async with session.post(
                    f"{self.ollama_url}/api/embeddings",
                    json={
                        "model": model_name,
                        "prompt": text[:800]  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª–∏–Ω—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    },
                    timeout=aiohttp.ClientTimeout(total=20)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        embedding = data.get('embedding', [])
                        if len(embedding) == self.embedding_dim:
                            return embedding
                    return None
            except Exception as e:
                return None
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        semaphore = asyncio.Semaphore(3)  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        async def bounded_fetch(text):
            async with semaphore:
                return await fetch_embedding(text)
        
        tasks = [bounded_fetch(text) for text in texts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception) or result is None:
                embeddings.append([0.0] * self.embedding_dim)
            else:
                embeddings.append(result)
        
        return np.array(embeddings, dtype=np.float32)
    
    def _split_text_into_chunks(self, text: str, chunk_size: int = 300, overlap: int = 30) -> list:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —á–∞–Ω–∫–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∞–±–∑–∞—Ü—ã
        paragraphs = text.split('\n\n')
        chunks = []
        
        for paragraph in paragraphs:
            if len(paragraph.strip()) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
                words = paragraph.split()
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ chunk_size —Å–ª–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∞–±–∑–∞—Ü–∞
                chunk = ' '.join(words[:chunk_size])
                chunks.append(chunk)
                
        return chunks[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    
    def build_knowledge_base(self, train_data_path: str):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        train_data = pd.read_csv(train_data_path)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        all_chunks = []
        for _, row in tqdm(train_data.iterrows(), total=len(train_data), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"):
            text = row['text']
            chunks = self._split_text_into_chunks(text)
            all_chunks.extend(chunks)
        
        self.documents = all_chunks
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        async def build_embeddings():
            async with aiohttp.ClientSession() as session:
                return await self.get_embeddings_async(all_chunks, session, "embeddinggemma:300m")
        
        embeddings = asyncio.run(build_embeddings())
        
        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype(np.float32))
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")
    
    async def search_relevant_documents_async(self, query: str, session: aiohttp.ClientSession, top_k: int = 5) -> list:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.index is None:
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self.get_embeddings_async([query], session, "embeddinggemma:300m")
            if len(query_embedding) == 0:
                return []
            
            faiss.normalize_L2(query_embedding)
            similarities, indices = self.index.search(query_embedding.astype(np.float32), top_k)
            
            relevant_docs = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.documents) and similarities[0][i] > 0.1:  # –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
                    relevant_docs.append(self.documents[idx])
            
            return relevant_docs
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

    async def generate_answer_async(self, question: str, context_docs: list, session: aiohttp.ClientSession) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        try:
            if not context_docs:
                return "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."
            
            # –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç
            context = "\n".join([f"- {doc}" for i, doc in enumerate(context_docs[:3])])
            
            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å:

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
- –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –¥–∞–π —á–∞—Å—Ç–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, —á—Ç–æ –µ—Å—Ç—å
- –ë—É–¥—å –ø–æ–ª–µ–∑–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º

–û—Ç–≤–µ—Ç:"""

            async with session.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": "gemma3:270m",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_predict": 400,
                        "top_k": 40,
                        "top_p": 0.9
                    }
                },
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    answer = result.get('response', '').strip()
                    
                    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
                    if len(answer) < 20 or "–Ω–µ –∑–Ω–∞—é" in answer.lower() or "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞" in answer.lower():
                        return "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–∞—Ç—å —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."
                    
                    return answer
                else:
                    return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"

    async def process_questions_batch(self, questions_batch: list, progress_callback=None) -> list:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        answers = []
        
        async with aiohttp.ClientSession() as session:
            for i, question in enumerate(questions_batch):
                try:
                    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    relevant_docs = await self.search_relevant_documents_async(question, session)
                    
                    # –î–ª—è –ø–µ—Ä–≤—ã—Ö 3 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–∫–∞–∂–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if i < 3:
                        print(f"\nüîç –û—Ç–ª–∞–¥–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ {i+1}:")
                        print(f"   –í–æ–ø—Ä–æ—Å: {question}")
                        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs)}")
                        if relevant_docs:
                            print(f"   –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: {relevant_docs[0][:100]}...")
                    
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                    answer = await self.generate_answer_async(question, relevant_docs, session)
                    answers.append(answer)
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if progress_callback:
                        progress_callback()
                        
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(0.3)
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
                    answers.append("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞")
                    if progress_callback:
                        progress_callback()
        
        return answers

def main():
    parser = argparse.ArgumentParser(description='–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è')
    parser.add_argument('--num_questions', type=int, default=500,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--ollama_url', type=str, default="http://localhost:11434",
                       help='URL Ollama —Å–µ—Ä–≤–µ—Ä–∞')
    parser.add_argument('--skip_build', action='store_true',
                       help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π')
    parser.add_argument('--batch_size', type=int, default=5,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ –§–ò–ù–ê–ù–°–û–í–´–ô –ê–°–°–ò–°–¢–ï–ù–¢ - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    assistant = FinancialAssistant(ollama_url=args.ollama_url)
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    if not args.skip_build:
        print("\nüì¶ –≠—Ç–∞–ø 1: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        build_start = time.time()
        assistant.build_knowledge_base('./train_data.csv')
        build_time = time.time() - build_start
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∑–∞ {build_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    print("\nüìã –≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤...")
    questions_df = pd.read_csv('./questions.csv')
    questions_list = questions_df['–í–æ–ø—Ä–æ—Å'].tolist()
    
    if args.num_questions > 0:
        questions_list = questions_list[:args.num_questions]
        questions_df = questions_df.head(args.num_questions)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions_list)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –±–∞—Ç—á–∞–º–∏
    print(f"\nüéØ –≠—Ç–∞–ø 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(questions_list)} –≤–æ–ø—Ä–æ—Å–æ–≤...")
    
    async def process_all_questions():
        all_answers = []
        total_questions = len(questions_list)
        
        with tqdm(total=total_questions, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤") as pbar:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
            for i in range(0, total_questions, args.batch_size):
                batch = questions_list[i:i + args.batch_size]
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
                batch_answers = await assistant.process_questions_batch(
                    batch, 
                    progress_callback=lambda: pbar.update(1)
                )
                all_answers.extend(batch_answers)
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                if i + args.batch_size < total_questions:
                    await asyncio.sleep(1)
        
        return all_answers
    
    process_start = time.time()
    answers_list = asyncio.run(process_all_questions())
    process_time = time.time() - process_start
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüíæ –≠—Ç–∞–ø 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    questions_df['–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å'] = answers_list
    output_file = f'submission_improved.csv'
    questions_df.to_csv(output_file, index=False, encoding='utf-8')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_time = process_time + (0 if args.skip_build else build_time)
    print("=" * 60)
    print("üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –í–æ–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(questions_list)}")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {process_time/60:.1f} –º–∏–Ω—É—Ç")
    if not args.skip_build:
        print(f"   ‚Ä¢ –í—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –±–∞–∑—ã: {build_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"   ‚Ä¢ –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_file}")
    print("=" * 60)
    
    # –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìù –ü—Ä–∏–º–µ—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤:")
    print("-" * 80)
    sample_size = min(5, len(questions_list))
    for i in range(sample_size):
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å {i+1}: {questions_list[i]}")
        print(f"üí° –û—Ç–≤–µ—Ç: {answers_list[i]}")
        print("-" * 80)

if __name__ == "__main__":
    main()